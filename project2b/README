Project 2B
==================
Zhehao Wang
404380075
zhehao@cs.ucla.edu

*** Contents ***

* lab2b.c      - the test driver
* SortedList.c - the list implementation
* SortedList.h - given list header
* Makefile
* README       - this file
* lab2b1-avg-time-vs-iterations.png   - graph 1
* 
* test.sh  - the test script (not required)

*** Answers ***

2B.1A. Explain the variation in time per operation vs the number of iterations?

Cost per op = (thread creation cost + list operations cost) / # of operations
At a low amount of number of iterations, thread creation overhead is much larger than list operations cost, and is the primary factor, which explains the first few data points being high, and then the average cost goes down as the thread creation overhead is amortized.
As the number of iterations goes up, the number of elements to be inserted, looked up and deleted goes up as well. Time complexity wise, inserting all is O(n^2), looking up all elements and then deleting them is O(n^2) as well. At a large number of operations, the list operation cost is much larger than thread creation cost, thus we see the rise in the later half of the graph.

2B.1B. How would you propose to correct for this effect?

One correction would be to graph (average cost per operation / number of elements) vs the number of operations.

2B.2A. Compare the variation in time per protected operation vs the number of threads in Project 2B and in Project 2A.  Explain the difference.

Linked list critical section is much longer than that of the add function, so the lock is held for a (potentially) longer time in 2b than in 2a; 
In 2b, the probability of conflict is much higher for the same number of threads. More conflicts means more blocked threads, more overhead, and less parallelism.

2B.0. Supplementary data on how many iterations it takes to fail consistently without yield.

Without synchronization and under different yield options, it take (approximately) the following amount of threads and iterations to fail consistently:

--yield=i  : 2 threads, 400 operations
--yield=d  : 2 threads, 600 operations
--yield=is : 2 threads, 200 operations
--yield=ds : 2 threads, 400 operations

*** Testing methodology ***

For the statistics collection, a shell script is created which runs the program repeatedly with different parameters.

All statistics were collected on seasnet lnxsrv07.

*** Note ***

The correction in 2nd graph is arguable. As the pseudo-randomized result has a (non-negligible) impact on the average time, the trend may not mean much; the general comparison between the two mechanisms could be slightly more meaningful.